{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import neuralcoref\n",
    "import re\n",
    "from newspaper import Article\n",
    "from googlesearch import search\n",
    "import nltk\n",
    "import json\n",
    "import itertools\n",
    "import tldextract\n",
    "import itertools\n",
    "from zeroshot_topics import ZeroShotTopicFinder\n",
    "from transformers import AutoTokenizer\n",
    "from zero_shot_re import RelTaggerModel, RelationExtractor\n",
    "from polyfuzz import PolyFuzz\n",
    "from transformers import pipeline\n",
    "from neo4j import GraphDatabase\n",
    "import hashlib\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import various nlp models for classifying data\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "nlp_two =spacy.load('en_core_web_sm')\n",
    "sentiment = pipeline(task = 'sentiment-analysis')\n",
    "zsmodel = ZeroShotTopicFinder()\n",
    "model = PolyFuzz(\"TF-IDF\")\n",
    "model_two = RelTaggerModel.from_pretrained(\"fractalego/fewrel-zero-shot\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to download articles \n",
    "## these could be amended to capture other data reddit/twitter/other social media\n",
    "## other text preprocessing can take place here as needed\n",
    "## tldextract extracts the root of the website so we document the sources\n",
    "\n",
    "def googlescrape(topic_name, num_results):\n",
    "    results = search(topic_name, num_results)\n",
    "    \n",
    "    sources = []\n",
    "    \n",
    "    \n",
    "    for r in results:\n",
    "        try:\n",
    "            url = r\n",
    "            article = Article(url, config=config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "            text = re.sub(r'http\\S+', '', text) #remove urls\n",
    "            text = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", text) #remove bracketed text\n",
    "            text = text.replace('()', \"\")\n",
    "            text = text.replace('[]', \"\")        \n",
    "            t = text.encode('ascii', 'ignore').decode() #remove unusual charachters\n",
    "            t = re.sub(\"https*\\S+\", \" \", t)\n",
    "            temp = []\n",
    "            for tem in t.split('\\n\\n'):\n",
    "                if len(tem.split(\" \")) > 8:\n",
    "                    temp.append(tem)\n",
    "            if len(\" \".join(temp)) < 500:\n",
    "                sources.append([t, \" \".join(temp), t.split('\\n\\n'), tldextract.extract(r).domain])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change for your search term and number of articles\n",
    "data = googlescrape(yoursearchtermhere, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "## enrich articles with sentiment analysis and zero-shot topics and creates a dataframe\n",
    "\n",
    "dflist = []\n",
    "print(len(data))\n",
    "for d in tqdm(data):\n",
    "    try:\n",
    "        topiclist = []\n",
    "        sentiment_list = []\n",
    "        for x in d[2]:\n",
    "            try:\n",
    "                for y in zsmodel.find_topic(x, n_topic=2):\n",
    "                    topiclist.append(y)\n",
    "                s = sentiment(x)\n",
    "                s = s[0]\n",
    "                sentiment_list.append(s['label'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        topic = max(set(topiclist), key=topiclist.count)\n",
    "        sentiment_value = max(set(sentiment_list), key=sentiment_list.count)\n",
    "\n",
    "        dflist.append([d[0], d[1], d[2], d[3], topic, sentiment_value])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(dflist, columns=['text', \"text_no_n\", 'text_list', 'domain', 'topic', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracts entity pairs from text\n",
    "\n",
    "def get_entity_pairs(text, coref=True):\n",
    "    # preprocess text\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "\n",
    "    def refine_ent(ent, sent):\n",
    "        unwanted_tokens = (\n",
    "            'PRON',  # pronouns\n",
    "            'PART',  # particle\n",
    "            'DET',  # determiner\n",
    "            'SCONJ',  # subordinating conjunction\n",
    "            'PUNCT',  # punctuation\n",
    "            'SYM',  # symbol\n",
    "            'X',  # other\n",
    "        )\n",
    "        ent_type = ent.ent_type_  # get entity type\n",
    "        if ent_type == '':\n",
    "            ent_type = 'NOUN_CHUNK'\n",
    "            ent = ' '.join(str(t.text) for t in\n",
    "                           nlp(str(ent)) if t.pos_\n",
    "                           not in unwanted_tokens and t.is_stop == False)\n",
    "        elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "            refined = ''\n",
    "            for i in range(len(sent) - ent.i):\n",
    "                if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                    refined += ' ' + str(ent.nbor(i))\n",
    "                else:\n",
    "                    ent = refined.strip()\n",
    "                    break\n",
    "\n",
    "        return ent, ent_type\n",
    "\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    ent_pairs = []\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span, attrs={'tag': span.root.tag,\n",
    "                                            'dep': span.root.dep}) for span in spans]\n",
    "        deps = [token.dep_ for token in sent]\n",
    "\n",
    "        # limit our example to simple sentences with one subject and object\n",
    "        if (deps.count('obj') + deps.count('dobj')) != 1\\\n",
    "                or (deps.count('subj') + deps.count('nsubj')) != 1:\n",
    "            continue\n",
    "\n",
    "        for token in sent:\n",
    "            if token.dep_ not in ('obj', 'dobj'):  # identify object nodes\n",
    "                continue\n",
    "            subject = [w for w in token.head.lefts if w.dep_\n",
    "                       in ('subj', 'nsubj')]  # identify subject nodes\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                # identify relationship by root dependency\n",
    "                relation = [w for w in token.ancestors if w.dep_ == 'ROOT']\n",
    "                if relation:\n",
    "                    relation = relation[0]\n",
    "                    # add adposition or particle to relationship\n",
    "                    if relation.nbor(1).pos_ in ('ADP', 'PART'):\n",
    "                        relation = ' '.join((str(relation), str(relation.nbor(1))))\n",
    "                else:\n",
    "                    relation = 'unknown'\n",
    "\n",
    "                subject, subject_type = refine_ent(subject, sent)\n",
    "                token, object_type = refine_ent(token, sent)\n",
    "\n",
    "                ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                  str(subject_type), str(object_type)])\n",
    "\n",
    "    ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(ent) == '' for ent in sublist)]\n",
    "    pairs = pd.DataFrame(ent_pairs, columns=['subject', 'relation', 'object',\n",
    "                                             'subject_type', 'object_type'])\n",
    "    print('Entity pairs extracted:', str(len(ent_pairs)))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates a column of entities extracted from text\n",
    "\n",
    "text_list = df['text_no_n'].tolist()\n",
    "entity_column = []\n",
    "\n",
    "for t in text_list:\n",
    "    text = nlp(t)\n",
    "    text = nlp(text._.coref_resolved)\n",
    "    shortlist = []\n",
    "    for ent in text.ents:\n",
    "        if ent.label_ in ['GPE', 'ORG', 'PERSON', 'NORP', 'MONEY', 'EVENT', 'WORK_OF_ART', 'FAC', 'NOUN_CHUNK']:\n",
    "            shortlist.append(ent.text)\n",
    "    \n",
    "    entity_column.append(shortlist)\n",
    "\n",
    "all_entities = list(set([item for sublist in entity_column for item in sublist]))\n",
    "all_entities.sort(key=lambda s: len(s))\n",
    "entity_column_corrected = []\n",
    "for e in entity_column:\n",
    "    entity_column_corrected.append(list(set(e)))\n",
    "df['entities'] = entity_column_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zero_shot extraction sometimes creates multiple names for the same entity despite coreference efforts\n",
    "# polyfuzz is a model which can help detect similar items and group the together\n",
    "model.match(all_entities)\n",
    "similar_words_df = model.get_matches()[model.get_matches().Similarity > 0.7]\n",
    "emptycounter = Counter()\n",
    "for x in [Counter(l) for l in df['entities'].tolist()]:\n",
    "    emptycounter.update(x)\n",
    "\n",
    "common_ents = dict(emptycounter)\n",
    "\n",
    "similar_dictionary = {}\n",
    "for index, row in similar_words_df.iterrows():\n",
    "    \n",
    "    if common_ents[row['From']] > common_ents[row['To']]:\n",
    "        similar_dictionary[row['From']] = row['To']\n",
    "    else:\n",
    "        if len(row['From']) < len(row['To']):\n",
    "            similar_dictionary[row['From']] = row['To']\n",
    "        else:\n",
    "            similar_dictionary[row['To']] = row['From']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove duplicates from entity column\n",
    "def deduplicator(ents):\n",
    "    new_ents = []\n",
    "    for x in ents:\n",
    "        \n",
    "        if x in similar_dictionary and (\"Â£\" or \"$\") not in dict(short_counter):\n",
    "            new_ents.append(similar_dictionary[x])\n",
    "        else:\n",
    "            new_ents.append(x)\n",
    "    return new_ents\n",
    "\n",
    "\n",
    "df['entities_corrected'] = df.apply(lambda x: deduplicator(x['entities']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up neo4j sandbox connection\n",
    "\n",
    "host = 'bolt://xx.xx.xx.xx:xxxx'\n",
    "user = 'xxxx'\n",
    "password = 'xxxx-xxxx-xxxx'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))\n",
    "\n",
    "def neo4j_query(query, params=None):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], columns=result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create hash column for display purposes\n",
    "df['hash'] = df.apply(lambda x: hash(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "## upload topics, domains, hashes and their relationships to neo4j sandbox instance\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    neo4j_query(\"\"\"\n",
    "    MERGE (a:Centre{centre:$centre})\n",
    "    MERGE (b:Topic{topic:$topic})\n",
    "    MERGE (c:Domain{domain:$domain})\n",
    "    MERGE (d:Hash{hash:$hash})\n",
    "    MERGE (a)-[:TOPIC]->(b)\n",
    "    MERGE (c)-[:COVERS]->(b)\n",
    "    MERGE (c)-[:MENTIONS]->(d)\n",
    "    \"\"\", {'centre':'1MDB', 'topic':row['topic'], 'domain':row['domain'], 'hash':row['hash']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link extracted entities to the article where they were mentioned\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    x = row['entities_corrected']\n",
    "    for y in x:\n",
    "        if y != '':\n",
    "            neo4j_query(\"\"\"\n",
    "            MERGE (x:Entity{entity:$entity})\n",
    "            MERGE (d:Hash{hash:$hash})\n",
    "            MERGE (d)-[:MENTIONS]->(x)\n",
    "            \"\"\", {'entity': y, 'hash':row['hash']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zero_shot relations to extract \n",
    "\n",
    "relations = ['linked'] ## this can be a list and any relationship you can think of however some work better than others\n",
    "extractor = RelationExtractor(model_two, tokenizer, relations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chunk text to be processed by model_two\n",
    "\n",
    "def get_chunks(s, maxlength):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while start + maxlength  < len(s) and end != -1:\n",
    "        end = s.rfind(\" \", start, start + maxlength + 1)\n",
    "        yield s[start:end]\n",
    "        start = end +1\n",
    "    yield s[start:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "## assemble list of assessed predictions from text\n",
    "\n",
    "predicted_rels = []\n",
    "for x in tqdm(range(0, len(df['text_no_n'].tolist()) - 1)):\n",
    "    for paragraph in get_chunks(df['text_no_n'].tolist()[x], 512):\n",
    "        combinations = list(itertools.combinations(list(set(df.iloc[x]['entities_corrected'])), 2))\n",
    "        try:\n",
    "            temp_df = get_entity_pairs(paragraph)\n",
    "            temp_df = temp_df[temp_df[\"subject_type\"].str.contains(\"NOUN_CHUNK\")==False] ## relationships with NOUN_CHUNK had high false positive rates \n",
    "            temp_df = temp_df[temp_df[\"object_type\"].str.contains(\"NOUN_CHUNK\")==False]\n",
    "            for index, row in temp_df.iterrows():\n",
    "                print(row['subject'])\n",
    "                print(row['object'])\n",
    "                predicted_rels.append({'head': row['subject'], 'tail': row['object'], 'type':row['relation'], 'source':str(df.iloc[x]['hash'])})\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        for combination in list(combinations):\n",
    "            try:\n",
    "                ranked_rels = extractor.rank(text=paragraph.replace(\",\", \" \"), head=combination[0], tail=combination[1])\n",
    "                if ranked_rels[0][1] > 0.8:\n",
    "                    print(combination)\n",
    "                    print(ranked_rels)\n",
    "                    predicted_rels.append({'head': combination[0], 'tail': combination[1], 'type':ranked_rels[0][0], 'source':str(df.iloc[x]['hash'])})\n",
    "\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "## upload predicted relationships\n",
    "\n",
    "neo4j_query(\"\"\"\n",
    "UNWIND $data as row\n",
    "MERGE (x:Entity{entity:row.head})\n",
    "MERGE (y:Entity{entity:row.tail})\n",
    "MERGE (d:Hash{hash:row.source})\n",
    "MERGE (x)-[:REL]->(r:Relation {type: row.type})-[:REL]->(y)\n",
    "MERGE (d)-[:MENTIONS]->(r)\n",
    "\"\"\", {'data': predicted_rels})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
